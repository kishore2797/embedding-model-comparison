# ğŸ”¬ Embedding Model Comparison

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.11+-3776AB?style=flat-square&logo=python&logoColor=white" />
  <img src="https://img.shields.io/badge/FastAPI-0.109+-009688?style=flat-square&logo=fastapi&logoColor=white" />
  <img src="https://img.shields.io/badge/React-19-61DAFB?style=flat-square&logo=react&logoColor=black" />
  <img src="https://img.shields.io/badge/FAISS-1.7+-blue?style=flat-square" />
</p>

> **Benchmark 11+ embedding models** â€” OpenAI, Cohere, and open-source (MiniLM, E5, BGE) on the same dataset. Compare retrieval accuracy, latency, cost, and embedding quality in one dashboard.

Part of the [Mastering RAG](https://github.com/kishore2797/mastering-rag) ecosystem â†’ tutorial: [rag-03-embedding-models](https://github.com/kishore2797/rag-03-embedding-models).

---

## ğŸŒ Real-World Scenario

> You're building a product search engine. Users type "lightweight laptop for programming under $800" and expect relevant results. The **embedding model** decides that "lightweight laptop" is close to "ultrabook" and "MacBook Air." Pick the wrong model and search returns desktops; pick an expensive one and you burn budget on a simple demo. This app lets you compare 11+ models on the same data and choose wisely.

---

## ğŸ—ï¸ What You'll Build

A benchmarking dashboard that compares **11+ embedding models** (OpenAI, Cohere, open-source) on the same datasets. Measure retrieval accuracy (Precision@K, MRR, NDCG), latency, cost, and embedding quality â€” with UMAP visualization and exportable reports.

```
Dataset (queries + ground truth) â”€â”€â†’ 11 Models in parallel
  â”œâ”€â”€ OpenAI text-embedding-3 (small, large, ada)
  â”œâ”€â”€ Cohere embed-v3 (english, multilingual, light)
  â””â”€â”€ Open-source: MiniLM, E5, BGE-large
â”€â”€â†’ Compare: Precision@K, MRR, NDCG, latency, cost, UMAP viz
```

## ğŸ”‘ Key Concepts

- **Embeddings** â€” Dense vectors that capture semantic meaning
- **Dimension trade-offs** â€” 384 vs. 768 vs. 1536 (speed vs. quality)
- **IR metrics** â€” Precision@K, Recall@K, MRR, NDCG, MAP, Hit Rate
- **Asymmetric embeddings** â€” Query and document can use different encoding strategies
- **Isotropy** â€” How uniformly embeddings are distributed in vector space

## ğŸ›  Tech Stack

| Layer | Technology |
|-------|------------|
| Backend | Python 3.11+ Â· FastAPI Â· sentence-transformers Â· FAISS Â· NumPy Â· scikit-learn |
| Frontend | React 19 Â· Vite Â· Tailwind CSS Â· Recharts Â· Lucide Icons |
| Models | OpenAI API Â· Cohere API Â· HuggingFace sentence-transformers |

## ğŸ“ Project Structure

```
embedding-model-comparison/
â”œâ”€â”€ backend/     # FastAPI: datasets, model runners, metrics, UMAP, export
â”œâ”€â”€ frontend/    # React + Vite: dataset/model selection, results tables, heatmaps, per-query analysis
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### Backend

```bash
cd backend
python3 -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
cp .env.example .env   # Optional: OpenAI, Cohere API keys for cloud models
uvicorn app.main:app --reload --port 8000
```

### Frontend

```bash
cd frontend
npm install
npm run dev
```

Open the app â€” select a dataset and embedding models, run the benchmark, view metrics and visualizations.

## âœ¨ Features

- **11+ embedding models** â€” OpenAI (3), Cohere (3), open-source: MiniLM, E5, BGE (5)
- **Built-in datasets** â€” Natural Questions, TechQA, Legal QA with ground-truth relevance
- **Full IR metrics** â€” Precision@K, Recall@K, MRR, NDCG@K, MAP, Hit Rate@K
- **Performance** â€” Latency percentiles, throughput, memory, API cost
- **Embedding quality** â€” Isotropy analysis, UMAP visualization
- **Interactive dashboard** â€” Radar charts, heatmaps, sortable tables, per-query analysis
- **Export** â€” Download results as JSON or Markdown
