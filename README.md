# ğŸ”¬ Embedding Model Comparison

> Benchmark embedding models (OpenAI, Cohere, and open-source) on the same dataset. Compare retrieval accuracy, latency, cost, and embedding quality â€” all from one interactive dashboard.

---

## âœ¨ Features

- ğŸ¤– **11 Embedding Models** â€” OpenAI (3), Cohere (3), open-source: MiniLM, E5, BGE (5)
- ğŸ“š **3 Built-in Datasets** â€” Natural Questions, TechQA, Legal QA with ground-truth relevance
- ğŸ“Š **Full IR Metrics** â€” Precision@K, Recall@K, MRR, NDCG@K, MAP, Hit Rate@K
- âš¡ **Performance Tracking** â€” Latency percentiles, throughput, memory usage, API cost
- ğŸ§¬ **Embedding Quality** â€” Isotropy analysis, UMAP visualization
- ğŸ“ˆ **Interactive Dashboard** â€” Radar charts, heatmaps, scatter plots, sortable tables
- ğŸ” **Per-Query Analysis** â€” Drill into where models agree/disagree
- ğŸ§ª **Live Query Explorer** â€” Test ad-hoc queries against cached embeddings
- ğŸ† **Weighted Ranking** â€” Customizable overall model ranking with adjustable weights
- ğŸ“¤ **Export Reports** â€” Download results as JSON or Markdown

---

## ğŸ“¸ Screenshots

### Setup & Run
Select a dataset and pick embedding models to benchmark. Local models run instantly â€” API models show cost estimates before running.

![Setup & Run](assets/image.png)

### Benchmark Results
Full retrieval accuracy table with sortable IR metrics, overall weighted ranking, and one-click report export.

![Benchmark Results](assets/image_1.png)

### Visualize â€” Metrics Heatmap
Models Ã— metrics color-coded matrix. Darker = better (except latency where lighter = better).

![Metrics Heatmap](assets/image_2.png)

### Per-Query Analysis
Filter queries by "all correct", "all wrong", or "models disagree" to find where models struggle.

![Per-Query Analysis](assets/image_3.png)

---

## ğŸ› ï¸ Tech Stack

| Layer | Technologies |
|-------|-------------|
| **Backend** | Python, FastAPI, sentence-transformers, FAISS, NumPy, scikit-learn |
| **Frontend** | React, Vite, Tailwind CSS, Recharts, Lucide Icons |
| **Search** | FAISS (cosine, dot-product, euclidean) |
| **Models** | OpenAI API, Cohere API, HuggingFace sentence-transformers |

---

## ğŸ“ Project Structure

```
embedding-model-comparison/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/routes/          # FastAPI route handlers
â”‚   â”‚   â”œâ”€â”€ embeddings/          # Provider wrappers (OpenAI, Cohere, local)
â”‚   â”‚   â”œâ”€â”€ benchmark/           # Runner, FAISS retrieval, cache
â”‚   â”‚   â”œâ”€â”€ evaluation/          # IR metrics, performance, embedding quality
â”‚   â”‚   â”œâ”€â”€ datasets/builtin/    # 3 built-in benchmark datasets
â”‚   â”‚   â”œâ”€â”€ models/schemas.py    # Pydantic models
â”‚   â”‚   â”œâ”€â”€ config.py            # Model registry & settings
â”‚   â”‚   â””â”€â”€ main.py              # FastAPI entry point
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ api/client.js        # Axios API client
â”‚   â”‚   â”œâ”€â”€ components/          # 10 React components
â”‚   â”‚   â”œâ”€â”€ App.jsx              # Main app layout with tab navigation
â”‚   â”‚   â””â”€â”€ main.jsx             # Entry point
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ assets/                      # Screenshots
â””â”€â”€ README.md
```

---

## ğŸš€ Getting Started

### Prerequisites

- ğŸ Python 3.10+
- ğŸ“¦ Node.js 18+

### Backend Setup

```bash
cd backend
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# Start the server
uvicorn app.main:app --reload --port 8000
```

### Frontend Setup

```bash
cd frontend
npm install
npm run dev
```

Open ğŸ‘‰ [http://localhost:5173](http://localhost:5173) in your browser.

### ğŸ”‘ API Keys (Optional)

For OpenAI and Cohere models, set environment variables:

```bash
export OPENAI_API_KEY="sk-..."
export COHERE_API_KEY="..."
```

> ğŸ’¡ Local models (MiniLM, E5, BGE) work without any API keys â€” perfect for getting started.

---

## ğŸ¤– Supported Models

| Model | Provider | Dimension | Cost |
|-------|----------|-----------|------|
| text-embedding-3-small | ğŸŸ¢ OpenAI | 1536 | $0.02/1M tokens |
| text-embedding-3-large | ğŸŸ¢ OpenAI | 3072 | $0.13/1M tokens |
| text-embedding-ada-002 | ğŸŸ¢ OpenAI | 1536 | $0.10/1M tokens |
| embed-english-v3.0 | ğŸŸ  Cohere | 1024 | $0.10/1M tokens |
| embed-english-light-v3.0 | ğŸŸ  Cohere | 384 | $0.10/1M tokens |
| embed-multilingual-v3.0 | ğŸŸ  Cohere | 1024 | $0.10/1M tokens |
| all-MiniLM-L6-v2 | ğŸ”µ Local | 384 | Free |
| e5-small-v2 | ğŸ”µ Local | 384 | Free |
| e5-base-v2 | ğŸ”µ Local | 768 | Free |
| bge-small-en-v1.5 | ğŸ”µ Local | 384 | Free |
| bge-base-en-v1.5 | ğŸ”µ Local | 768 | Free |

---

## ğŸŒ API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/api/models` | ğŸ“‹ List available embedding models |
| `POST` | `/api/models/validate` | âœ… Check model accessibility |
| `GET` | `/api/datasets` | ğŸ“š List benchmark datasets |
| `GET` | `/api/datasets/:id` | ğŸ“„ Get dataset with documents & queries |
| `POST` | `/api/datasets/build` | ğŸ—ï¸ Create custom dataset |
| `POST` | `/api/benchmark/run` | â–¶ï¸ Start a benchmark run |
| `GET` | `/api/benchmark/status/:id` | ğŸ“Š Get benchmark progress |
| `POST` | `/api/benchmark/cancel/:id` | â¹ï¸ Cancel a running benchmark |
| `GET` | `/api/results/:run_id` | ğŸ“ˆ Get full benchmark results |
| `GET` | `/api/results/:run_id/embeddings` | ğŸ§¬ Get embedding quality analysis |
| `GET` | `/api/results/:run_id/umap` | ğŸ—ºï¸ Get UMAP coordinates |
| `POST` | `/api/results/:run_id/export` | ğŸ“¤ Export report (JSON/Markdown) |
| `POST` | `/api/explore/query` | ğŸ” Live query against cached embeddings |
| `POST` | `/api/explore/similarity` | ğŸ”— Compare text similarity across models |
| `GET` | `/api/health` | ğŸ’š Health check |

---

## ğŸ“– Usage

1. **ğŸ“š Select a dataset** â€” Choose from 3 built-in benchmark datasets with ground-truth relevance
2. **ğŸ¤– Pick models** â€” Select 1â€“6 embedding models to compare (local models need no API keys)
3. **â–¶ï¸ Run benchmark** â€” Click "Start Benchmark" and watch real-time progress with ETA
4. **ğŸ“Š Analyze results** â€” Switch between Accuracy, Performance, Radar, Recall@K, and Scatter tabs
5. **ğŸ† Compare rankings** â€” View weighted overall ranking with customizable metric weights
6. **ğŸ”¥ Visualize** â€” Explore the metrics heatmap, UMAP embedding space, and similarity inspector
7. **ğŸ” Drill into queries** â€” Filter by "all correct", "all wrong", or "models disagree"
8. **ğŸ§ª Explore live** â€” Type custom queries to see how each model retrieves documents
9. **ğŸ“¤ Export** â€” Download your benchmark report as JSON or Markdown

---

## ğŸ“„ License

MIT
